{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# An√°lise de S√©ries Temporais - Dados Clim√°ticos NOAA\n",
        "\n",
        "## Projeto de Previs√£o de Temperaturas - S√£o Paulo, Brasil\n",
        "\n",
        "Este notebook realiza uma an√°lise completa de s√©ries temporais utilizando dados clim√°ticos da NOAA, incluindo:\n",
        "- Coleta e explora√ß√£o de dados\n",
        "- Prepara√ß√£o e tratamento\n",
        "- Modelagem preditiva com m√∫ltiplos modelos\n",
        "- Avalia√ß√£o e compara√ß√£o de modelos\n",
        "- Dashboard interativo\n",
        "\n",
        "**Fonte dos Dados:** NOAA GHCNh (Global Historical Climatology Network - Hourly)\n",
        "**Localiza√ß√£o:** S√£o Paulo, Brasil\n",
        "**Formato:** PSV (Pipe-Separated Values) - Dados hor√°rios agregados em di√°rios\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
          ]
        }
      ],
      "source": [
        "# Importa√ß√£o de bibliotecas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.express as px\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√µes de visualiza√ß√£o\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Bibliotecas para modelagem\n",
        "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# Prophet\n",
        "from prophet import Prophet\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# TensorFlow/Keras para LSTM\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Auto ARIMA\n",
        "try:\n",
        "    from pmdarima import auto_arima\n",
        "    AUTO_ARIMA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    AUTO_ARIMA_AVAILABLE = False\n",
        "    print(\"pmdarima n√£o dispon√≠vel. Instale com: pip install pmdarima\")\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")\n",
        "print(f\"TensorFlow vers√£o: {tf.__version__}\")\n",
        "print(f\"Pandas vers√£o: {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Etapa 1: Coleta e Explora√ß√£o de Dados (EDA)\n",
        "\n",
        "## 1.1. Aquisi√ß√£o dos Dados\n",
        "\n",
        "Vamos carregar dados clim√°ticos do arquivo PSV local. Utilizaremos dados de temperatura de S√£o Paulo, Brasil, do GHCNh (Global Historical Climatology Network - Hourly).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fun√ß√£o para carregar dados do arquivo PSV local (S√£o Paulo, Brasil)\n",
        "# Usa apenas arquivo local, sem conex√£o com internet\n",
        "\n",
        "import os\n",
        "\n",
        "def load_ghcnh_psv(file_path='data/GHCNh_AAI0000TNCA_2025.psv'):\n",
        "    \"\"\"\n",
        "    Carrega e processa arquivo PSV do GHCNh (Global Historical Climatology Network - Hourly).\n",
        "    \n",
        "    Esta fun√ß√£o:\n",
        "    1. L√™ o arquivo PSV (pipe-separated values)\n",
        "    2. Converte dados hor√°rios em dados di√°rios (m√©dia, min, max)\n",
        "    3. Retorna DataFrame pronto para an√°lise\n",
        "    \n",
        "    Par√¢metros:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        Caminho para o arquivo .psv (padr√£o: data/GHCNh_AAI0000TNCA_2025.psv)\n",
        "        \n",
        "    Retorna:\n",
        "    --------\n",
        "    pd.DataFrame com dados processados (di√°rio) ou None se falhar\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"CARREGAMENTO DE DADOS PSV - GHCNh\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"\\nArquivo: {file_path}\")\n",
        "        \n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"\\n‚ùå Erro: Arquivo n√£o encontrado: {file_path}\")\n",
        "            print(\"\\nPor favor, certifique-se de que o arquivo PSV est√° na pasta 'data/'\")\n",
        "            return None\n",
        "        \n",
        "        print(f\"‚úì Arquivo encontrado!\")\n",
        "        print(f\"\\nLendo arquivo PSV...\")\n",
        "        \n",
        "        # Ler arquivo PSV (pipe-separated values)\n",
        "        df = pd.read_csv(file_path, sep='|', low_memory=False)\n",
        "        print(f\"  Total de registros hor√°rios: {len(df):,}\")\n",
        "        \n",
        "        # Informa√ß√µes sobre a esta√ß√£o\n",
        "        if 'Station_name' in df.columns and len(df) > 0:\n",
        "            station_name = df['Station_name'].iloc[0]\n",
        "            print(f\"  Esta√ß√£o: {station_name}\")\n",
        "        \n",
        "        if 'STATION' in df.columns and len(df) > 0:\n",
        "            station_code = df['STATION'].iloc[0]\n",
        "            print(f\"  C√≥digo: {station_code}\")\n",
        "        \n",
        "        # Converter coluna DATE para datetime\n",
        "        if 'DATE' not in df.columns:\n",
        "            print(\"‚ùå Erro: Coluna 'DATE' n√£o encontrada no arquivo\")\n",
        "            return None\n",
        "        \n",
        "        print(f\"  Convertendo datas...\")\n",
        "        df['DATE'] = pd.to_datetime(df['DATE'])\n",
        "        \n",
        "        # Informa√ß√µes sobre o per√≠odo\n",
        "        print(f\"  Per√≠odo: {df['DATE'].min()} a {df['DATE'].max()}\")\n",
        "        \n",
        "        # Verificar coluna de temperatura\n",
        "        if 'temperature' not in df.columns:\n",
        "            print(\"‚ùå Erro: Coluna 'temperature' n√£o encontrada no arquivo\")\n",
        "            return None\n",
        "        \n",
        "        # Filtrar valores v√°lidos de temperatura\n",
        "        df_valid = df[df['temperature'].notna()].copy()\n",
        "        print(f\"  Registros com temperatura v√°lida: {len(df_valid):,} ({100*len(df_valid)/len(df):.1f}%)\")\n",
        "        \n",
        "        if len(df_valid) == 0:\n",
        "            print(\"‚ùå Erro: Nenhum registro v√°lido de temperatura encontrado\")\n",
        "            return None\n",
        "        \n",
        "        print(f\"\\nAgregando dados hor√°rios para di√°rios...\")\n",
        "        \n",
        "        # Agregar dados hor√°rios para di√°rios\n",
        "        # Criar coluna de data (sem hora)\n",
        "        df_valid['date'] = df_valid['DATE'].dt.date\n",
        "        \n",
        "        # Agregar por dia\n",
        "        daily_agg = {\n",
        "            'temperature': ['mean', 'min', 'max'],\n",
        "        }\n",
        "        \n",
        "        # Adicionar outras colunas se dispon√≠veis\n",
        "        if 'precipitation' in df_valid.columns:\n",
        "            daily_agg['precipitation'] = lambda x: x.sum() if x.notna().any() else np.nan\n",
        "        \n",
        "        if 'wind_speed' in df_valid.columns:\n",
        "            daily_agg['wind_speed'] = 'mean'\n",
        "        \n",
        "        if 'relative_humidity' in df_valid.columns:\n",
        "            daily_agg['relative_humidity'] = 'mean'\n",
        "        \n",
        "        daily_df = df_valid.groupby('date').agg(daily_agg).reset_index()\n",
        "        \n",
        "        # Flatten column names\n",
        "        new_columns = ['date']\n",
        "        for col in daily_agg.keys():\n",
        "            if col == 'temperature':\n",
        "                new_columns.extend(['temperature', 'temp_min', 'temp_max'])\n",
        "            else:\n",
        "                new_columns.append(col)\n",
        "        \n",
        "        daily_df.columns = new_columns\n",
        "        \n",
        "        # Converter date de date para datetime\n",
        "        daily_df['date'] = pd.to_datetime(daily_df['date'])\n",
        "        \n",
        "        # Selecionar colunas principais\n",
        "        result_df = daily_df[['date', 'temperature']].copy()\n",
        "        \n",
        "        # Remover linhas onde temperatura m√©dia √© NaN\n",
        "        result_df = result_df[result_df['temperature'].notna()].copy()\n",
        "        result_df = result_df.sort_values('date').reset_index(drop=True)\n",
        "        \n",
        "        print(f\"‚úì Dados processados: {len(result_df):,} dias\")\n",
        "        print(f\"  Per√≠odo: {result_df['date'].min()} a {result_df['date'].max()}\")\n",
        "        print(f\"  Temperatura m√©dia: {result_df['temperature'].mean():.2f} ¬∞C\")\n",
        "        print(f\"  Temperatura m√≠nima: {result_df['temperature'].min():.2f} ¬∞C\")\n",
        "        print(f\"  Temperatura m√°xima: {result_df['temperature'].max():.2f} ¬∞C\")\n",
        "        \n",
        "        return result_df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Erro ao processar arquivo PSV: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Carregar dados do arquivo PSV local\n",
        "PSV_FILE = 'data/GHCNh_AAI0000TNCA_2025.psv'  # Arquivo PSV de S√£o Paulo\n",
        "CSV_FILE = 'noaa_data.csv'  # Arquivo CSV processado (se existir)\n",
        "\n",
        "# Prioridade 1: Arquivo PSV local (dados hor√°rios de S√£o Paulo)\n",
        "if os.path.exists(PSV_FILE):\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"CARREGANDO DADOS DO ARQUIVO PSV LOCAL\")\n",
        "    print(\"=\" * 80)\n",
        "    df = load_ghcnh_psv(PSV_FILE)\n",
        "    \n",
        "    if df is not None and len(df) > 0:\n",
        "        # Salvar em CSV para uso futuro (opcional)\n",
        "        df.to_csv(CSV_FILE, index=False)\n",
        "        print(f\"\\n‚úì Dados salvos tamb√©m em: {CSV_FILE}\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Erro: N√£o foi poss√≠vel processar o arquivo PSV\")\n",
        "        df = None\n",
        "\n",
        "# Prioridade 2: Arquivo CSV processado (fallback)\n",
        "elif os.path.exists(CSV_FILE):\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CARREGANDO DADOS DE ARQUIVO CSV PROCESSADO\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Arquivo encontrado: {CSV_FILE}\\n\")\n",
        "    df = pd.read_csv(CSV_FILE)\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    print(f\"‚úì Dados carregados: {len(df)} registros\")\n",
        "    print(f\"  Per√≠odo: {df['date'].min()} a {df['date'].max()}\")\n",
        "\n",
        "# Se nenhum arquivo encontrado\n",
        "else:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"‚ùå ERRO: NENHUM ARQUIVO DE DADOS ENCONTRADO\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nPor favor, certifique-se de que o arquivo PSV est√° em: {PSV_FILE}\")\n",
        "    print(\"Ou execute primeiro: python3 download_data.py\")\n",
        "    print(\"\\nArquivo esperado:\")\n",
        "    print(f\"  {PSV_FILE}\")\n",
        "    df = None\n",
        "\n",
        "# Verificar se os dados foram carregados com sucesso\n",
        "if df is not None and len(df) > 0:\n",
        "    # Resumo final dos dados\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"RESUMO DOS DADOS CARREGADOS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"üìç Localiza√ß√£o: S√£o Paulo, Brasil\")\n",
        "    print(f\"Total de registros: {len(df):,}\")\n",
        "    print(f\"Per√≠odo: {df['date'].min()} a {df['date'].max()}\")\n",
        "    print(f\"Valores ausentes: {df['temperature'].isnull().sum()} ({df['temperature'].isnull().sum()/len(df)*100:.2f}%)\")\n",
        "    print(f\"\\nEstat√≠sticas b√°sicas:\")\n",
        "    print(df['temperature'].describe())\n",
        "    \n",
        "    print(\"\\nPrimeiras linhas:\")\n",
        "    print(df.head(10))\n",
        "    print(\"\\n√öltimas linhas:\")\n",
        "    print(df.tail(10))\n",
        "    print(\"\\nInforma√ß√µes do dataset:\")\n",
        "    print(df.info())\n",
        "else:\n",
        "    print(\"\\n‚ùå N√£o foi poss√≠vel carregar os dados. Verifique o arquivo PSV.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2. An√°lise Explorat√≥ria de Dados (EDA)\n",
        "\n",
        "### Estat√≠sticas Descritivas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estat√≠sticas descritivas\n",
        "print(\"=\" * 60)\n",
        "print(\"ESTAT√çSTICAS DESCRITIVAS\")\n",
        "print(\"=\" * 60)\n",
        "print(df.describe())\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"VERIFICA√á√ÉO DE VALORES AUSENTES\")\n",
        "print(\"=\" * 60)\n",
        "print(df.isnull().sum())\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"VERIFICA√á√ÉO DE DUPLICATAS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"N√∫mero de duplicatas: {df.duplicated().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualiza√ß√µes Iniciais\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar data como √≠ndice\n",
        "df_ts = df.set_index('date').copy()\n",
        "\n",
        "# Visualiza√ß√£o da s√©rie temporal completa\n",
        "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
        "\n",
        "# Temperatura - s√©rie completa\n",
        "axes[0].plot(df_ts.index, df_ts['temperature'], linewidth=0.5, alpha=0.7, color='red')\n",
        "axes[0].set_title('S√©rie Temporal de Temperatura (¬∞C) - Per√≠odo Completo', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Temperatura (¬∞C)')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Temperatura - zoom nos √∫ltimos 2 anos\n",
        "last_2_years = df_ts.tail(730)  # Aproximadamente 2 anos\n",
        "axes[1].plot(last_2_years.index, last_2_years['temperature'], linewidth=1, alpha=0.8, color='darkred')\n",
        "axes[1].set_title('S√©rie Temporal de Temperatura (¬∞C) - √öltimos 2 Anos (Zoom)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Temperatura (¬∞C)')\n",
        "axes[1].set_xlabel('Data')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribui√ß√µes e an√°lises estat√≠sticas\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# Histograma\n",
        "axes[0, 0].hist(df_ts['temperature'], bins=50, color='red', alpha=0.7, edgecolor='black')\n",
        "axes[0, 0].set_title('Distribui√ß√£o de Temperatura', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Temperatura (¬∞C)')\n",
        "axes[0, 0].set_ylabel('Frequ√™ncia')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Boxplot\n",
        "axes[0, 1].boxplot(df_ts['temperature'], vert=True)\n",
        "axes[0, 1].set_title('Boxplot - Temperatura', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Temperatura (¬∞C)')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Densidade (KDE)\n",
        "df_ts['temperature'].plot.density(ax=axes[1, 0], color='red', linewidth=2)\n",
        "axes[1, 0].set_title('Densidade de Probabilidade (KDE)', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Temperatura (¬∞C)')\n",
        "axes[1, 0].set_ylabel('Densidade')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Q-Q Plot para normalidade\n",
        "from scipy import stats\n",
        "stats.probplot(df_ts['temperature'].dropna(), dist=\"norm\", plot=axes[1, 1])\n",
        "axes[1, 1].set_title('Q-Q Plot (Normalidade)', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise de sazonalidade - temperatura m√©dia por m√™s\n",
        "df_ts['month'] = df_ts.index.month\n",
        "df_ts['year'] = df_ts.index.year\n",
        "\n",
        "# Temperatura m√©dia mensal\n",
        "monthly_temp = df_ts.groupby('month')['temperature'].mean()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Temperatura m√©dia por m√™s\n",
        "axes[0].plot(monthly_temp.index, monthly_temp.values, marker='o', linewidth=2, markersize=8, color='red')\n",
        "axes[0].set_title('Temperatura M√©dia Mensal', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('M√™s')\n",
        "axes[0].set_ylabel('Temperatura M√©dia (¬∞C)')\n",
        "axes[0].set_xticks(range(1, 13))\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Temperatura m√©dia por ano\n",
        "yearly_temp = df_ts.groupby('year')['temperature'].mean()\n",
        "axes[1].plot(yearly_temp.index, yearly_temp.values, marker='o', linewidth=2, markersize=8, color='darkred')\n",
        "axes[1].set_title('Temperatura M√©dia Anual (Tend√™ncia)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Ano')\n",
        "axes[1].set_ylabel('Temperatura M√©dia (¬∞C)')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Temperatura m√©dia mensal:\")\n",
        "print(monthly_temp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decomposi√ß√£o da S√©rie Temporal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decomposi√ß√£o aditiva da s√©rie temporal de temperatura\n",
        "decomposition = seasonal_decompose(df_ts['temperature'], model='additive', period=365)\n",
        "\n",
        "fig, axes = plt.subplots(4, 1, figsize=(16, 12))\n",
        "\n",
        "decomposition.observed.plot(ax=axes[0], color='blue', linewidth=0.5)\n",
        "axes[0].set_title('S√©rie Original', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Temperatura (¬∞C)')\n",
        "\n",
        "decomposition.trend.plot(ax=axes[1], color='green', linewidth=1.5)\n",
        "axes[1].set_title('Tend√™ncia', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Temperatura (¬∞C)')\n",
        "\n",
        "decomposition.seasonal.plot(ax=axes[2], color='orange', linewidth=0.5)\n",
        "axes[2].set_title('Sazonalidade', fontsize=12, fontweight='bold')\n",
        "axes[2].set_ylabel('Temperatura (¬∞C)')\n",
        "\n",
        "decomposition.resid.plot(ax=axes[3], color='red', linewidth=0.5)\n",
        "axes[3].set_title('Res√≠duos', fontsize=12, fontweight='bold')\n",
        "axes[3].set_ylabel('Temperatura (¬∞C)')\n",
        "axes[3].set_xlabel('Data')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Teste de Estacionariedade (Dickey-Fuller)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Teste de estacionariedade\n",
        "def test_stationarity(timeseries):\n",
        "    \"\"\"\n",
        "    Realiza o teste de Dickey-Fuller aumentado para verificar estacionariedade\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TESTE DE ESTACIONARIEDADE - DICKEY-FULLER AUMENTADO\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    dftest = adfuller(timeseries.dropna(), autolag='AIC')\n",
        "    dfoutput = pd.Series(dftest[0:4], index=['Estat√≠stica do Teste', 'p-value', \n",
        "                                              '#Lags Usados', 'N√∫mero de Observa√ß√µes'])\n",
        "    \n",
        "    for key, value in dftest[4].items():\n",
        "        dfoutput[f'Valor Cr√≠tico ({key})'] = value\n",
        "    \n",
        "    print(dfoutput)\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    if dftest[1] <= 0.05:\n",
        "        print(\"Resultado: S√©rie √© ESTACION√ÅRIA (rejeita H0)\")\n",
        "    else:\n",
        "        print(\"Resultado: S√©rie N√ÉO √© estacion√°ria (n√£o rejeita H0)\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return dftest[1] <= 0.05\n",
        "\n",
        "# Testar estacionariedade da s√©rie original\n",
        "is_stationary = test_stationarity(df_ts['temperature'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Etapa 2: Prepara√ß√£o e Tratamento dos Dados\n",
        "\n",
        "## 2.1. Limpeza de Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Focaremos na vari√°vel temperatura para previs√£o\n",
        "# Criar c√≥pia para trabalhar\n",
        "df_clean = df_ts[['temperature']].copy()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LIMPEZA DE DADOS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Valores ausentes antes: {df_clean['temperature'].isnull().sum()}\")\n",
        "print(f\"Percentual de valores ausentes: {df_clean['temperature'].isnull().sum() / len(df_clean) * 100:.2f}%\")\n",
        "\n",
        "# Tratamento de valores ausentes (interpola√ß√£o linear)\n",
        "if df_clean['temperature'].isnull().sum() > 0:\n",
        "    df_clean['temperature'] = df_clean['temperature'].interpolate(method='linear')\n",
        "    print(f\"Valores ausentes ap√≥s interpola√ß√£o: {df_clean['temperature'].isnull().sum()}\")\n",
        "\n",
        "# Identifica√ß√£o de outliers usando IQR\n",
        "Q1 = df_clean['temperature'].quantile(0.25)\n",
        "Q3 = df_clean['temperature'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "outliers = df_clean[(df_clean['temperature'] < lower_bound) | (df_clean['temperature'] > upper_bound)]\n",
        "print(f\"\\nOutliers identificados (m√©todo IQR): {len(outliers)} ({len(outliers)/len(df_clean)*100:.2f}%)\")\n",
        "\n",
        "# Visualizar outliers\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "axes[0].boxplot(df_clean['temperature'], vert=True)\n",
        "axes[0].set_title('Boxplot - Identifica√ß√£o de Outliers', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Temperatura (¬∞C)')\n",
        "\n",
        "axes[1].plot(df_clean.index, df_clean['temperature'], linewidth=0.5, alpha=0.7, label='Temperatura')\n",
        "axes[1].scatter(outliers.index, outliers['temperature'], color='red', s=10, alpha=0.5, label='Outliers')\n",
        "axes[1].set_title('S√©rie Temporal com Outliers Destacados', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Temperatura (¬∞C)')\n",
        "axes[1].set_xlabel('Data')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Decis√£o: Manter outliers pois podem ser valores clim√°ticos v√°lidos\n",
        "# Em um caso real, voc√™ analisaria se s√£o erros de medi√ß√£o ou valores extremos leg√≠timos\n",
        "print(\"\\nDecis√£o: Mantendo outliers (podem representar eventos clim√°ticos extremos v√°lidos)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2. Transforma√ß√£o dos Dados e Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar features temporais\n",
        "df_features = df_clean.copy()\n",
        "\n",
        "# Features temporais\n",
        "df_features['day_of_week'] = df_features.index.dayofweek\n",
        "df_features['day_of_month'] = df_features.index.day\n",
        "df_features['month'] = df_features.index.month\n",
        "df_features['quarter'] = df_features.index.quarter\n",
        "df_features['year'] = df_features.index.year\n",
        "df_features['day_of_year'] = df_features.index.dayofyear\n",
        "\n",
        "# Features c√≠clicas (para capturar sazonalidade)\n",
        "df_features['month_sin'] = np.sin(2 * np.pi * df_features['month'] / 12)\n",
        "df_features['month_cos'] = np.cos(2 * np.pi * df_features['month'] / 12)\n",
        "df_features['day_of_year_sin'] = np.sin(2 * np.pi * df_features['day_of_year'] / 365.25)\n",
        "df_features['day_of_year_cos'] = np.cos(2 * np.pi * df_features['day_of_year'] / 365.25)\n",
        "\n",
        "# Lags (valores anteriores)\n",
        "df_features['lag_1'] = df_features['temperature'].shift(1)\n",
        "df_features['lag_7'] = df_features['temperature'].shift(7)  # Semana anterior\n",
        "df_features['lag_30'] = df_features['temperature'].shift(30)  # M√™s anterior\n",
        "df_features['lag_365'] = df_features['temperature'].shift(365)  # Ano anterior\n",
        "\n",
        "# M√©dias m√≥veis\n",
        "df_features['ma_7'] = df_features['temperature'].rolling(window=7).mean()\n",
        "df_features['ma_30'] = df_features['temperature'].rolling(window=30).mean()\n",
        "df_features['ma_365'] = df_features['temperature'].rolling(window=365).mean()\n",
        "\n",
        "# Diferen√ßa para tornar estacion√°ria (se necess√°rio)\n",
        "df_features['temperature_diff'] = df_features['temperature'].diff()\n",
        "\n",
        "print(\"Features criadas:\")\n",
        "print(df_features.columns.tolist())\n",
        "print(f\"\\nShape do dataset: {df_features.shape}\")\n",
        "print(\"\\nPrimeiras linhas:\")\n",
        "print(df_features.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3. Divis√£o dos Dados (Train/Validation/Test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Divis√£o temporal dos dados (respeitando ordem temporal)\n",
        "# Remover NaN criados por lags e m√©dias m√≥veis\n",
        "df_model = df_features.dropna().copy()\n",
        "\n",
        "total_size = len(df_model)\n",
        "train_size = int(total_size * 0.6)  # 60% para treino\n",
        "val_size = int(total_size * 0.2)    # 20% para valida√ß√£o\n",
        "# 20% restante para teste\n",
        "\n",
        "# Divis√£o temporal\n",
        "train_data = df_model.iloc[:train_size]\n",
        "val_data = df_model.iloc[train_size:train_size + val_size]\n",
        "test_data = df_model.iloc[train_size + val_size:]\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DIVIS√ÉO DOS DADOS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total de registros: {total_size}\")\n",
        "print(f\"Treino: {len(train_data)} ({len(train_data)/total_size*100:.1f}%)\")\n",
        "print(f\"Valida√ß√£o: {len(val_data)} ({len(val_data)/total_size*100:.1f}%)\")\n",
        "print(f\"Teste: {len(test_data)} ({len(test_data)/total_size*100:.1f}%)\")\n",
        "print(f\"\\nPer√≠odo de treino: {train_data.index.min()} a {train_data.index.max()}\")\n",
        "print(f\"Per√≠odo de valida√ß√£o: {val_data.index.min()} a {val_data.index.max()}\")\n",
        "print(f\"Per√≠odo de teste: {test_data.index.min()} a {test_data.index.max()}\")\n",
        "\n",
        "# Visualizar divis√£o\n",
        "fig, ax = plt.subplots(figsize=(16, 6))\n",
        "ax.plot(train_data.index, train_data['temperature'], label='Treino', color='blue', linewidth=0.5)\n",
        "ax.plot(val_data.index, val_data['temperature'], label='Valida√ß√£o', color='orange', linewidth=0.5)\n",
        "ax.plot(test_data.index, test_data['temperature'], label='Teste', color='red', linewidth=0.5)\n",
        "ax.set_title('Divis√£o Temporal dos Dados', fontsize=14, fontweight='bold')\n",
        "ax.set_ylabel('Temperatura (¬∞C)')\n",
        "ax.set_xlabel('Data')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# S√©ries para modelagem (apenas temperatura)\n",
        "y_train = train_data['temperature']\n",
        "y_val = val_data['temperature']\n",
        "y_test = test_data['temperature']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Etapa 3: Modelagem Preditiva\n",
        "\n",
        "## 3.1. Modelos Baseline\n",
        "\n",
        "Vamos come√ßar com modelos simples que servir√£o como baseline para compara√ß√£o.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fun√ß√£o para calcular m√©tricas\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calcula m√©tricas de avalia√ß√£o para modelos de previs√£o\n",
        "    \"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    \n",
        "    return {\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse,\n",
        "        'MAPE': mape,\n",
        "        'R¬≤': r2\n",
        "    }\n",
        "\n",
        "# Armazenar resultados de todos os modelos\n",
        "results = {}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 1: Naive Forecast (√öltimo Valor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Naive Forecast: prev√™ o √∫ltimo valor observado\n",
        "naive_pred_val = np.full(len(y_val), y_train.iloc[-1])\n",
        "naive_pred_test = np.full(len(y_test), y_val.iloc[-1])\n",
        "\n",
        "results['Naive'] = {\n",
        "    'val': calculate_metrics(y_val, naive_pred_val),\n",
        "    'test': calculate_metrics(y_test, naive_pred_test)\n",
        "}\n",
        "\n",
        "print(\"Modelo Naive (Baseline)\")\n",
        "print(\"=\" * 40)\n",
        "print(\"Valida√ß√£o:\")\n",
        "for metric, value in results['Naive']['val'].items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n",
        "print(\"\\nTeste:\")\n",
        "for metric, value in results['Naive']['test'].items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 2: M√©dia M√≥vel Simples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# M√©dia m√≥vel simples (janela de 30 dias)\n",
        "window = 30\n",
        "ma_val = y_train.rolling(window=window).mean().iloc[-1]\n",
        "ma_test = pd.concat([y_train, y_val]).rolling(window=window).mean().iloc[-1]\n",
        "\n",
        "ma_pred_val = np.full(len(y_val), ma_val)\n",
        "ma_pred_test = np.full(len(y_test), ma_test)\n",
        "\n",
        "results['M√©dia M√≥vel'] = {\n",
        "    'val': calculate_metrics(y_val, ma_pred_val),\n",
        "    'test': calculate_metrics(y_test, ma_pred_test)\n",
        "}\n",
        "\n",
        "print(\"Modelo M√©dia M√≥vel (30 dias)\")\n",
        "print(\"=\" * 40)\n",
        "print(\"Valida√ß√£o:\")\n",
        "for metric, value in results['M√©dia M√≥vel']['val'].items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n",
        "print(\"\\nTeste:\")\n",
        "for metric, value in results['M√©dia M√≥vel']['test'].items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 3: Suaviza√ß√£o Exponencial Simples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Suaviza√ß√£o exponencial simples\n",
        "# Usando a m√©dia dos √∫ltimos valores com peso exponencial\n",
        "alpha = 0.3  # Par√¢metro de suaviza√ß√£o\n",
        "\n",
        "# Calcular valor suavizado final do treino\n",
        "exp_smooth_val = y_train.ewm(alpha=alpha, adjust=False).mean().iloc[-1]\n",
        "exp_smooth_test = pd.concat([y_train, y_val]).ewm(alpha=alpha, adjust=False).mean().iloc[-1]\n",
        "\n",
        "exp_pred_val = np.full(len(y_val), exp_smooth_val)\n",
        "exp_pred_test = np.full(len(y_test), exp_smooth_test)\n",
        "\n",
        "results['Suaviza√ß√£o Exponencial'] = {\n",
        "    'val': calculate_metrics(y_val, exp_pred_val),\n",
        "    'test': calculate_metrics(y_test, exp_pred_test)\n",
        "}\n",
        "\n",
        "print(\"Modelo Suaviza√ß√£o Exponencial Simples\")\n",
        "print(\"=\" * 40)\n",
        "print(\"Valida√ß√£o:\")\n",
        "for metric, value in results['Suaviza√ß√£o Exponencial']['val'].items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n",
        "print(\"\\nTeste:\")\n",
        "for metric, value in results['Suaviza√ß√£o Exponencial']['test'].items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2. Modelos Avan√ßados\n",
        "\n",
        "### Modelo 4: ARIMA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ARIMA - Auto ARIMA se dispon√≠vel, sen√£o ARIMA manual\n",
        "print(\"Treinando modelo ARIMA...\")\n",
        "\n",
        "if AUTO_ARIMA_AVAILABLE:\n",
        "    # Auto ARIMA para encontrar melhores par√¢metros\n",
        "    auto_model = auto_arima(y_train, \n",
        "                           seasonal=False,\n",
        "                           stepwise=True,\n",
        "                           suppress_warnings=True,\n",
        "                           error_action='ignore',\n",
        "                           max_p=5, max_q=5, max_d=2,\n",
        "                           trace=False)\n",
        "    \n",
        "    arima_order = auto_model.order\n",
        "    print(f\"Melhores par√¢metros ARIMA encontrados: {arima_order}\")\n",
        "    \n",
        "    # Treinar modelo final\n",
        "    arima_model = ARIMA(y_train, order=arima_order)\n",
        "    arima_fitted = arima_model.fit()\n",
        "else:\n",
        "    # ARIMA manual (1,1,1) - valores comuns\n",
        "    arima_order = (1, 1, 1)\n",
        "    print(f\"Usando par√¢metros ARIMA: {arima_order}\")\n",
        "    arima_model = ARIMA(y_train, order=arima_order)\n",
        "    arima_fitted = arima_model.fit()\n",
        "\n",
        "# Previs√µes\n",
        "arima_pred_val = arima_fitted.forecast(steps=len(y_val))\n",
        "arima_pred_test = arima_fitted.forecast(steps=len(y_test))\n",
        "\n",
        "# Ajustar para usar dados de valida√ß√£o no teste\n",
        "# Re-treinar com treino + valida√ß√£o para previs√£o no teste\n",
        "arima_model_full = ARIMA(pd.concat([y_train, y_val]), order=arima_order)\n",
        "arima_fitted_full = arima_model_full.fit()\n",
        "arima_pred_test = arima_fitted_full.forecast(steps=len(y_test))\n",
        "\n",
        "results['ARIMA'] = {\n",
        "    'val': calculate_metrics(y_val, arima_pred_val),\n",
        "    'test': calculate_metrics(y_test, arima_pred_test),\n",
        "    'order': arima_order\n",
        "}\n",
        "\n",
        "print(\"\\nModelo ARIMA\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Par√¢metros: {arima_order}\")\n",
        "print(\"Valida√ß√£o:\")\n",
        "for metric, value in results['ARIMA']['val'].items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n",
        "print(\"\\nTeste:\")\n",
        "for metric, value in results['ARIMA']['test'].items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 5: SARIMA (Seasonal ARIMA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SARIMA - ARIMA sazonal\n",
        "print(\"Treinando modelo SARIMA...\")\n",
        "\n",
        "# Par√¢metros SARIMA: (p,d,q)(P,D,Q,s)\n",
        "# s = 365 para sazonalidade anual\n",
        "sarima_order = (1, 1, 1)\n",
        "seasonal_order = (1, 1, 1, 365)\n",
        "\n",
        "try:\n",
        "    sarima_model = SARIMAX(y_train, \n",
        "                          order=sarima_order,\n",
        "                          seasonal_order=seasonal_order,\n",
        "                          enforce_stationarity=False,\n",
        "                          enforce_invertibility=False)\n",
        "    sarima_fitted = sarima_model.fit(disp=False, maxiter=50)\n",
        "    \n",
        "    # Previs√µes\n",
        "    sarima_pred_val = sarima_fitted.forecast(steps=len(y_val))\n",
        "    \n",
        "    # Re-treinar para teste\n",
        "    sarima_model_full = SARIMAX(pd.concat([y_train, y_val]),\n",
        "                                order=sarima_order,\n",
        "                                seasonal_order=seasonal_order,\n",
        "                                enforce_stationarity=False,\n",
        "                                enforce_invertibility=False)\n",
        "    sarima_fitted_full = sarima_model_full.fit(disp=False, maxiter=50)\n",
        "    sarima_pred_test = sarima_fitted_full.forecast(steps=len(y_test))\n",
        "    \n",
        "    results['SARIMA'] = {\n",
        "        'val': calculate_metrics(y_val, sarima_pred_val),\n",
        "        'test': calculate_metrics(y_test, sarima_pred_test),\n",
        "        'order': (sarima_order, seasonal_order)\n",
        "    }\n",
        "    \n",
        "    print(\"\\nModelo SARIMA\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Par√¢metros: {sarima_order}, Sazonal: {seasonal_order}\")\n",
        "    print(\"Valida√ß√£o:\")\n",
        "    for metric, value in results['SARIMA']['val'].items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "    print(\"\\nTeste:\")\n",
        "    for metric, value in results['SARIMA']['test'].items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Erro ao treinar SARIMA: {e}\")\n",
        "    print(\"Usando valores nulos para SARIMA\")\n",
        "    results['SARIMA'] = {\n",
        "        'val': {'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan, 'R¬≤': np.nan},\n",
        "        'test': {'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan, 'R¬≤': np.nan}\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 6: Holt-Winters (Triple Exponential Smoothing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Holt-Winters (Triple Exponential Smoothing)\n",
        "print(\"Treinando modelo Holt-Winters...\")\n",
        "\n",
        "try:\n",
        "    # Modelo aditivo com sazonalidade anual\n",
        "    hw_model = ExponentialSmoothing(y_train,\n",
        "                                   seasonal_periods=365,\n",
        "                                   trend='add',\n",
        "                                   seasonal='add',\n",
        "                                   initialization_method='estimated')\n",
        "    hw_fitted = hw_model.fit(optimized=True)\n",
        "    \n",
        "    # Previs√µes\n",
        "    hw_pred_val = hw_fitted.forecast(steps=len(y_val))\n",
        "    \n",
        "    # Re-treinar para teste\n",
        "    hw_model_full = ExponentialSmoothing(pd.concat([y_train, y_val]),\n",
        "                                        seasonal_periods=365,\n",
        "                                        trend='add',\n",
        "                                        seasonal='add',\n",
        "                                        initialization_method='estimated')\n",
        "    hw_fitted_full = hw_model_full.fit(optimized=True)\n",
        "    hw_pred_test = hw_fitted_full.forecast(steps=len(y_test))\n",
        "    \n",
        "    results['Holt-Winters'] = {\n",
        "        'val': calculate_metrics(y_val, hw_pred_val),\n",
        "        'test': calculate_metrics(y_test, hw_pred_test)\n",
        "    }\n",
        "    \n",
        "    print(\"\\nModelo Holt-Winters\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"Valida√ß√£o:\")\n",
        "    for metric, value in results['Holt-Winters']['val'].items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "    print(\"\\nTeste:\")\n",
        "    for metric, value in results['Holt-Winters']['test'].items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Erro ao treinar Holt-Winters: {e}\")\n",
        "    # Tentar com per√≠odo sazonal menor\n",
        "    try:\n",
        "        hw_model = ExponentialSmoothing(y_train,\n",
        "                                       seasonal_periods=12,  # Mensal\n",
        "                                       trend='add',\n",
        "                                       seasonal='add',\n",
        "                                       initialization_method='estimated')\n",
        "        hw_fitted = hw_model.fit(optimized=True)\n",
        "        hw_pred_val = hw_fitted.forecast(steps=len(y_val))\n",
        "        \n",
        "        hw_model_full = ExponentialSmoothing(pd.concat([y_train, y_val]),\n",
        "                                            seasonal_periods=12,\n",
        "                                            trend='add',\n",
        "                                            seasonal='add',\n",
        "                                            initialization_method='estimated')\n",
        "        hw_fitted_full = hw_model_full.fit(optimized=True)\n",
        "        hw_pred_test = hw_fitted_full.forecast(steps=len(y_test))\n",
        "        \n",
        "        results['Holt-Winters'] = {\n",
        "            'val': calculate_metrics(y_val, hw_pred_val),\n",
        "            'test': calculate_metrics(y_test, hw_pred_test)\n",
        "        }\n",
        "        \n",
        "        print(\"\\nModelo Holt-Winters (per√≠odo sazonal ajustado)\")\n",
        "        print(\"=\" * 40)\n",
        "        print(\"Valida√ß√£o:\")\n",
        "        for metric, value in results['Holt-Winters']['val'].items():\n",
        "            print(f\"  {metric}: {value:.4f}\")\n",
        "        print(\"\\nTeste:\")\n",
        "        for metric, value in results['Holt-Winters']['test'].items():\n",
        "            print(f\"  {metric}: {value:.4f}\")\n",
        "    except Exception as e2:\n",
        "        print(f\"Erro ao treinar Holt-Winters com per√≠odo ajustado: {e2}\")\n",
        "        results['Holt-Winters'] = {\n",
        "            'val': {'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan, 'R¬≤': np.nan},\n",
        "            'test': {'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan, 'R¬≤': np.nan}\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 7: Prophet (Facebook Prophet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prophet requer formato espec√≠fico: ds (data) e y (valor)\n",
        "print(\"Treinando modelo Prophet...\")\n",
        "\n",
        "# Preparar dados para Prophet\n",
        "prophet_train = pd.DataFrame({\n",
        "    'ds': y_train.index,\n",
        "    'y': y_train.values\n",
        "})\n",
        "\n",
        "prophet_val = pd.DataFrame({\n",
        "    'ds': y_val.index,\n",
        "    'y': y_val.values\n",
        "})\n",
        "\n",
        "prophet_test = pd.DataFrame({\n",
        "    'ds': y_test.index,\n",
        "    'y': y_test.values\n",
        "})\n",
        "\n",
        "try:\n",
        "    # Treinar modelo Prophet\n",
        "    prophet_model = Prophet(\n",
        "        yearly_seasonality=True,\n",
        "        weekly_seasonality=True,\n",
        "        daily_seasonality=False,\n",
        "        seasonality_mode='additive'\n",
        "    )\n",
        "    prophet_model.fit(prophet_train)\n",
        "    \n",
        "    # Previs√µes para valida√ß√£o\n",
        "    prophet_future_val = prophet_model.make_future_dataframe(periods=len(y_val))\n",
        "    prophet_forecast_val = prophet_model.predict(prophet_future_val)\n",
        "    prophet_pred_val = prophet_forecast_val.tail(len(y_val))['yhat'].values\n",
        "    \n",
        "    # Re-treinar para teste\n",
        "    prophet_train_full = pd.concat([\n",
        "        pd.DataFrame({'ds': y_train.index, 'y': y_train.values}),\n",
        "        pd.DataFrame({'ds': y_val.index, 'y': y_val.values})\n",
        "    ])\n",
        "    \n",
        "    prophet_model_full = Prophet(\n",
        "        yearly_seasonality=True,\n",
        "        weekly_seasonality=True,\n",
        "        daily_seasonality=False,\n",
        "        seasonality_mode='additive'\n",
        "    )\n",
        "    prophet_model_full.fit(prophet_train_full)\n",
        "    \n",
        "    prophet_future_test = prophet_model_full.make_future_dataframe(periods=len(y_test))\n",
        "    prophet_forecast_test = prophet_model_full.predict(prophet_future_test)\n",
        "    prophet_pred_test = prophet_forecast_test.tail(len(y_test))['yhat'].values\n",
        "    \n",
        "    results['Prophet'] = {\n",
        "        'val': calculate_metrics(y_val, prophet_pred_val),\n",
        "        'test': calculate_metrics(y_test, prophet_pred_test)\n",
        "    }\n",
        "    \n",
        "    print(\"\\nModelo Prophet\")\n",
        "    print(\"=\" * 40)\n",
        "    print(\"Valida√ß√£o:\")\n",
        "    for metric, value in results['Prophet']['val'].items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "    print(\"\\nTeste:\")\n",
        "    for metric, value in results['Prophet']['test'].items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Erro ao treinar Prophet: {e}\")\n",
        "    results['Prophet'] = {\n",
        "        'val': {'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan, 'R¬≤': np.nan},\n",
        "        'test': {'MAE': np.nan, 'RMSE': np.nan, 'MAPE': np.nan, 'R¬≤': np.nan}\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo 8: LSTM (Long Short-Term Memory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM - Prepara√ß√£o dos dados\n",
        "print(\"Preparando dados para LSTM...\")\n",
        "\n",
        "def create_sequences(data, seq_length=60):\n",
        "    \"\"\"\n",
        "    Cria sequ√™ncias para LSTM\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Normalizar dados\n",
        "scaler_lstm = MinMaxScaler()\n",
        "y_train_scaled = scaler_lstm.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_val_scaled = scaler_lstm.transform(y_val.values.reshape(-1, 1))\n",
        "y_test_scaled = scaler_lstm.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Combinar treino e valida√ß√£o para criar sequ√™ncias\n",
        "train_val_combined = np.concatenate([y_train_scaled, y_val_scaled])\n",
        "\n",
        "# Criar sequ√™ncias\n",
        "seq_length = 60  # 60 dias de hist√≥rico\n",
        "X_train, y_train_seq = create_sequences(train_val_combined, seq_length)\n",
        "X_test, y_test_seq = create_sequences(\n",
        "    np.concatenate([train_val_combined, y_test_scaled]), seq_length\n",
        ")\n",
        "\n",
        "# Ajustar para que o teste use apenas dados futuros\n",
        "# Separar treino e teste corretamente\n",
        "train_end_idx = len(train_val_combined) - seq_length\n",
        "X_train_final = X_train[:train_end_idx]\n",
        "y_train_final = y_train_seq[:train_end_idx]\n",
        "\n",
        "# Para teste, usar sequ√™ncias que terminam no conjunto de teste\n",
        "test_start_idx = len(train_val_combined) - seq_length\n",
        "X_test_final = []\n",
        "y_test_final = []\n",
        "\n",
        "for i in range(len(y_test_scaled)):\n",
        "    if test_start_idx + i < len(X_train):\n",
        "        X_test_final.append(X_train[test_start_idx + i])\n",
        "        y_test_final.append(y_test_scaled[i])\n",
        "\n",
        "X_test_final = np.array(X_test_final)\n",
        "y_test_final = np.array(y_test_final)\n",
        "\n",
        "print(f\"Shape X_train: {X_train_final.shape}, y_train: {y_train_final.shape}\")\n",
        "print(f\"Shape X_test: {X_test_final.shape}, y_test: {y_test_final.shape}\")\n",
        "\n",
        "# Reshape para LSTM [samples, time_steps, features]\n",
        "X_train_final = X_train_final.reshape((X_train_final.shape[0], X_train_final.shape[1], 1))\n",
        "X_test_final = X_test_final.reshape((X_test_final.shape[0], X_test_final.shape[1], 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construir modelo LSTM\n",
        "print(\"Treinando modelo LSTM...\")\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    LSTM(50, activation='relu', return_sequences=True, input_shape=(seq_length, 1)),\n",
        "    Dropout(0.2),\n",
        "    LSTM(50, activation='relu', return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Callback para early stopping\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Treinar modelo\n",
        "history = lstm_model.fit(\n",
        "    X_train_final, y_train_final,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Previs√µes\n",
        "lstm_pred_test_scaled = lstm_model.predict(X_test_final, verbose=0)\n",
        "lstm_pred_test = scaler_lstm.inverse_transform(lstm_pred_test_scaled).flatten()\n",
        "\n",
        "# Para valida√ß√£o, usar uma abordagem similar\n",
        "# Criar sequ√™ncias de valida√ß√£o\n",
        "val_start_idx = len(y_train_scaled) - seq_length\n",
        "X_val_final = []\n",
        "y_val_final = []\n",
        "\n",
        "for i in range(len(y_val_scaled)):\n",
        "    if val_start_idx + i < len(X_train):\n",
        "        X_val_final.append(X_train[val_start_idx + i])\n",
        "        y_val_final.append(y_val_scaled[i])\n",
        "\n",
        "X_val_final = np.array(X_val_final).reshape((len(X_val_final), seq_length, 1))\n",
        "lstm_pred_val_scaled = lstm_model.predict(X_val_final, verbose=0)\n",
        "lstm_pred_val = scaler_lstm.inverse_transform(lstm_pred_val_scaled).flatten()\n",
        "\n",
        "results['LSTM'] = {\n",
        "    'val': calculate_metrics(y_val[:len(lstm_pred_val)], lstm_pred_val),\n",
        "    'test': calculate_metrics(y_test[:len(lstm_pred_test)], lstm_pred_test)\n",
        "}\n",
        "\n",
        "print(\"\\nModelo LSTM\")\n",
        "print(\"=\" * 40)\n",
        "print(\"Valida√ß√£o:\")\n",
        "for metric, value in results['LSTM']['val'].items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n",
        "print(\"\\nTeste:\")\n",
        "for metric, value in results['LSTM']['test'].items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "# Visualizar hist√≥rico de treinamento\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "axes[0].plot(history.history['loss'], label='Loss Treino')\n",
        "axes[0].plot(history.history['val_loss'], label='Loss Valida√ß√£o')\n",
        "axes[0].set_title('Hist√≥rico de Loss - LSTM')\n",
        "axes[0].set_xlabel('√âpoca')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(history.history['mae'], label='MAE Treino')\n",
        "axes[1].plot(history.history['val_mae'], label='MAE Valida√ß√£o')\n",
        "axes[1].set_title('Hist√≥rico de MAE - LSTM')\n",
        "axes[1].set_xlabel('√âpoca')\n",
        "axes[1].set_ylabel('MAE')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Etapa 4: Avalia√ß√£o e Compara√ß√£o de Modelos\n",
        "\n",
        "## 4.1. Tabela Comparativa de M√©tricas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar tabela comparativa\n",
        "comparison_data = []\n",
        "\n",
        "for model_name, model_results in results.items():\n",
        "    if 'val' in model_results and 'test' in model_results:\n",
        "        comparison_data.append({\n",
        "            'Modelo': model_name,\n",
        "            'MAE (Val)': model_results['val'].get('MAE', np.nan),\n",
        "            'RMSE (Val)': model_results['val'].get('RMSE', np.nan),\n",
        "            'MAPE (Val)': model_results['val'].get('MAPE', np.nan),\n",
        "            'R¬≤ (Val)': model_results['val'].get('R¬≤', np.nan),\n",
        "            'MAE (Test)': model_results['test'].get('MAE', np.nan),\n",
        "            'RMSE (Test)': model_results['test'].get('RMSE', np.nan),\n",
        "            'MAPE (Test)': model_results['test'].get('MAPE', np.nan),\n",
        "            'R¬≤ (Test)': model_results['test'].get('R¬≤', np.nan)\n",
        "        })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"TABELA COMPARATIVA DE MODELOS\")\n",
        "print(\"=\" * 100)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Salvar tabela\n",
        "comparison_df.to_csv('comparacao_modelos.csv', index=False)\n",
        "print(\"\\nTabela salva em 'comparacao_modelos.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza√ß√£o comparativa das m√©tricas\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "\n",
        "# Filtrar modelos v√°lidos\n",
        "valid_models = comparison_df.dropna(subset=['MAE (Test)'])\n",
        "\n",
        "# MAE\n",
        "axes[0, 0].bar(valid_models['Modelo'], valid_models['MAE (Test)'], color='skyblue', edgecolor='black')\n",
        "axes[0, 0].set_title('MAE (Mean Absolute Error) - Conjunto de Teste', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('MAE')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# RMSE\n",
        "axes[0, 1].bar(valid_models['Modelo'], valid_models['RMSE (Test)'], color='lightcoral', edgecolor='black')\n",
        "axes[0, 1].set_title('RMSE (Root Mean Squared Error) - Conjunto de Teste', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('RMSE')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# MAPE\n",
        "axes[1, 0].bar(valid_models['Modelo'], valid_models['MAPE (Test)'], color='lightgreen', edgecolor='black')\n",
        "axes[1, 0].set_title('MAPE (Mean Absolute Percentage Error) - Conjunto de Teste', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('MAPE (%)')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# R¬≤\n",
        "axes[1, 1].bar(valid_models['Modelo'], valid_models['R¬≤ (Test)'], color='plum', edgecolor='black')\n",
        "axes[1, 1].set_title('R¬≤ (Coeficiente de Determina√ß√£o) - Conjunto de Teste', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('R¬≤')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2. Visualiza√ß√µes de Previs√µes vs. Valores Reais\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Armazenar todas as previs√µes para visualiza√ß√£o\n",
        "predictions = {\n",
        "    'Naive': {'val': naive_pred_val, 'test': naive_pred_test},\n",
        "    'M√©dia M√≥vel': {'val': ma_pred_val, 'test': ma_pred_test},\n",
        "    'Suaviza√ß√£o Exponencial': {'val': exp_pred_val, 'test': exp_pred_test},\n",
        "    'ARIMA': {'val': arima_pred_val, 'test': arima_pred_test},\n",
        "    'LSTM': {'val': lstm_pred_val, 'test': lstm_pred_test}\n",
        "}\n",
        "\n",
        "# Adicionar outras previs√µes se dispon√≠veis\n",
        "if 'SARIMA' in results and not np.isnan(results['SARIMA']['test']['MAE']):\n",
        "    try:\n",
        "        predictions['SARIMA'] = {'val': sarima_pred_val, 'test': sarima_pred_test}\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "if 'Prophet' in results and not np.isnan(results['Prophet']['test']['MAE']):\n",
        "    try:\n",
        "        predictions['Prophet'] = {'val': prophet_pred_val, 'test': prophet_pred_test}\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "if 'Holt-Winters' in results and not np.isnan(results['Holt-Winters']['test']['MAE']):\n",
        "    try:\n",
        "        predictions['Holt-Winters'] = {'val': hw_pred_val, 'test': hw_pred_test}\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Visualizar previs√µes no conjunto de teste\n",
        "n_models = len([k for k in predictions.keys() if 'test' in predictions[k]])\n",
        "fig, axes = plt.subplots(n_models, 1, figsize=(18, 4*n_models))\n",
        "\n",
        "if n_models == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "idx = 0\n",
        "for model_name in predictions.keys():\n",
        "    if 'test' in predictions[model_name]:\n",
        "        ax = axes[idx]\n",
        "        \n",
        "        # Ajustar tamanho se necess√°rio\n",
        "        test_pred = predictions[model_name]['test']\n",
        "        if len(test_pred) > len(y_test):\n",
        "            test_pred = test_pred[:len(y_test)]\n",
        "        elif len(test_pred) < len(y_test):\n",
        "            y_test_plot = y_test[:len(test_pred)]\n",
        "        else:\n",
        "            y_test_plot = y_test\n",
        "        \n",
        "        if len(test_pred) != len(y_test_plot):\n",
        "            min_len = min(len(test_pred), len(y_test_plot))\n",
        "            test_pred = test_pred[:min_len]\n",
        "            y_test_plot = y_test_plot[:min_len]\n",
        "        \n",
        "        ax.plot(y_test_plot.index, y_test_plot.values, label='Valores Reais', \n",
        "                linewidth=1.5, alpha=0.7, color='blue')\n",
        "        ax.plot(y_test_plot.index, test_pred, label='Previs√µes', \n",
        "                linewidth=1.5, alpha=0.7, color='red', linestyle='--')\n",
        "        ax.set_title(f'{model_name} - Previs√µes vs. Valores Reais (Teste)', \n",
        "                    fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('Temperatura (¬∞C)')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        idx += 1\n",
        "\n",
        "plt.xlabel('Data')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3. An√°lise de Res√≠duos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise de res√≠duos para o melhor modelo\n",
        "# Identificar melhor modelo por R¬≤ no teste\n",
        "best_model_name = valid_models.loc[valid_models['R¬≤ (Test)'].idxmax(), 'Modelo']\n",
        "print(f\"Melhor modelo (por R¬≤): {best_model_name}\")\n",
        "\n",
        "# Calcular res√≠duos do melhor modelo\n",
        "if best_model_name in predictions and 'test' in predictions[best_model_name]:\n",
        "    best_pred = predictions[best_model_name]['test']\n",
        "    if len(best_pred) > len(y_test):\n",
        "        best_pred = best_pred[:len(y_test)]\n",
        "    elif len(best_pred) < len(y_test):\n",
        "        y_test_resid = y_test[:len(best_pred)]\n",
        "    else:\n",
        "        y_test_resid = y_test\n",
        "    \n",
        "    if len(best_pred) != len(y_test_resid):\n",
        "        min_len = min(len(best_pred), len(y_test_resid))\n",
        "        best_pred = best_pred[:min_len]\n",
        "        y_test_resid = y_test_resid[:min_len]\n",
        "    \n",
        "    residuals = y_test_resid.values - best_pred\n",
        "    \n",
        "    # Visualizar res√≠duos\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    \n",
        "    # Res√≠duos ao longo do tempo\n",
        "    axes[0, 0].plot(y_test_resid.index, residuals, linewidth=0.5, alpha=0.7, color='red')\n",
        "    axes[0, 0].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "    axes[0, 0].set_title(f'Res√≠duos ao Longo do Tempo - {best_model_name}', \n",
        "                         fontsize=12, fontweight='bold')\n",
        "    axes[0, 0].set_ylabel('Res√≠duos')\n",
        "    axes[0, 0].set_xlabel('Data')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Histograma de res√≠duos\n",
        "    axes[0, 1].hist(residuals, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "    axes[0, 1].set_title('Distribui√ß√£o dos Res√≠duos', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Res√≠duos')\n",
        "    axes[0, 1].set_ylabel('Frequ√™ncia')\n",
        "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Q-Q plot (normalidade)\n",
        "    from scipy import stats\n",
        "    stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
        "    axes[1, 0].set_title('Q-Q Plot (Normalidade dos Res√≠duos)', \n",
        "                         fontsize=12, fontweight='bold')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Res√≠duos vs. Valores Previstos\n",
        "    axes[1, 1].scatter(best_pred, residuals, alpha=0.5, s=10, color='blue')\n",
        "    axes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=1)\n",
        "    axes[1, 1].set_title('Res√≠duos vs. Valores Previstos', fontsize=12, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Valores Previstos')\n",
        "    axes[1, 1].set_ylabel('Res√≠duos')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Estat√≠sticas dos res√≠duos\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"AN√ÅLISE DE RES√çDUOS - {best_model_name}\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"M√©dia dos res√≠duos: {np.mean(residuals):.4f}\")\n",
        "    print(f\"Desvio padr√£o dos res√≠duos: {np.std(residuals):.4f}\")\n",
        "    print(f\"Teste de normalidade (Shapiro-Wilk):\")\n",
        "    from scipy.stats import shapiro\n",
        "    stat, p_value = shapiro(residuals[:5000])  # Limitar para n√£o exceder tamanho m√°ximo\n",
        "    print(f\"  Estat√≠stica: {stat:.4f}, p-value: {p_value:.4f}\")\n",
        "    if p_value > 0.05:\n",
        "        print(\"  Res√≠duos s√£o normalmente distribu√≠dos (n√£o rejeita H0)\")\n",
        "    else:\n",
        "        print(\"  Res√≠duos N√ÉO s√£o normalmente distribu√≠dos (rejeita H0)\")\n",
        "    \n",
        "    # Teste de autocorrela√ß√£o dos res√≠duos\n",
        "    from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "    lb_test = acorr_ljungbox(residuals[:1000], lags=10, return_df=True)\n",
        "    print(f\"\\nTeste de Ljung-Box (autocorrela√ß√£o):\")\n",
        "    print(f\"  p-value (lag 10): {lb_test['lb_pvalue'].iloc[-1]:.4f}\")\n",
        "    if lb_test['lb_pvalue'].iloc[-1] > 0.05:\n",
        "        print(\"  N√£o h√° autocorrela√ß√£o significativa nos res√≠duos\")\n",
        "    else:\n",
        "        print(\"  H√° autocorrela√ß√£o significativa nos res√≠duos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.4. Sele√ß√£o do Modelo Final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise para sele√ß√£o do melhor modelo\n",
        "print(\"=\" * 80)\n",
        "print(\"SELE√á√ÉO DO MODELO FINAL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Melhor modelo por cada m√©trica\n",
        "print(\"\\nMelhor modelo por m√©trica (conjunto de teste):\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "metrics_to_check = ['MAE (Test)', 'RMSE (Test)', 'MAPE (Test)', 'R¬≤ (Test)']\n",
        "best_by_metric = {}\n",
        "\n",
        "for metric in metrics_to_check:\n",
        "    if metric == 'R¬≤ (Test)':\n",
        "        # Para R¬≤, maior √© melhor\n",
        "        best_idx = valid_models[metric].idxmax()\n",
        "        best_by_metric[metric] = valid_models.loc[best_idx, 'Modelo']\n",
        "        best_value = valid_models.loc[best_idx, metric]\n",
        "    else:\n",
        "        # Para MAE, RMSE, MAPE, menor √© melhor\n",
        "        best_idx = valid_models[metric].idxmin()\n",
        "        best_by_metric[metric] = valid_models.loc[best_idx, 'Modelo']\n",
        "        best_value = valid_models.loc[best_idx, metric]\n",
        "    \n",
        "    print(f\"{metric:20s}: {best_by_metric[metric]:20s} (valor: {best_value:.4f})\")\n",
        "\n",
        "# Modelo com melhor R¬≤ (geralmente o mais importante)\n",
        "best_model_final = valid_models.loc[valid_models['R¬≤ (Test)'].idxmax(), 'Modelo']\n",
        "best_r2 = valid_models.loc[valid_models['R¬≤ (Test)'].idxmax(), 'R¬≤ (Test)']\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"MODELO FINAL SELECIONADO: {best_model_final}\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nJustificativa:\")\n",
        "print(f\"- R¬≤ no conjunto de teste: {best_r2:.4f}\")\n",
        "print(f\"- Este modelo apresenta o melhor ajuste aos dados (maior R¬≤)\")\n",
        "print(f\"- Considerando todas as m√©tricas, este modelo oferece o melhor equil√≠brio\")\n",
        "\n",
        "# Mostrar todas as m√©tricas do modelo final\n",
        "final_metrics = valid_models[valid_models['Modelo'] == best_model_final].iloc[0]\n",
        "print(f\"\\nM√©tricas completas do modelo final:\")\n",
        "print(f\"  MAE (Test):  {final_metrics['MAE (Test)']:.4f}\")\n",
        "print(f\"  RMSE (Test): {final_metrics['RMSE (Test)']:.4f}\")\n",
        "print(f\"  MAPE (Test): {final_metrics['MAPE (Test)']:.4f}\")\n",
        "print(f\"  R¬≤ (Test):   {final_metrics['R¬≤ (Test)']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"LIMITA√á√ïES E PONTOS FORTES\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nPontos fortes do {best_model_final}:\")\n",
        "if 'LSTM' in best_model_final:\n",
        "    print(\"- Captura padr√µes n√£o-lineares complexos\")\n",
        "    print(\"- Boa capacidade de aprendizado de sequ√™ncias longas\")\n",
        "    print(\"- Adapta-se bem a padr√µes sazonais\")\n",
        "elif 'Prophet' in best_model_final:\n",
        "    print(\"- Excelente para dados com sazonalidade\")\n",
        "    print(\"- Lida bem com feriados e eventos especiais\")\n",
        "    print(\"- Interpretabilidade das componentes\")\n",
        "elif 'ARIMA' in best_model_final or 'SARIMA' in best_model_final:\n",
        "    print(\"- Baseado em fundamentos estat√≠sticos s√≥lidos\")\n",
        "    print(\"- Interpretabilidade dos par√¢metros\")\n",
        "    print(\"- Boa performance em s√©ries estacion√°rias\")\n",
        "else:\n",
        "    print(\"- Simplicidade e facilidade de interpreta√ß√£o\")\n",
        "    print(\"- Baixo custo computacional\")\n",
        "    print(\"- Boa performance como baseline\")\n",
        "\n",
        "print(f\"\\nLimita√ß√µes do {best_model_final}:\")\n",
        "if 'LSTM' in best_model_final:\n",
        "    print(\"- Requer grande quantidade de dados\")\n",
        "    print(\"- Alto custo computacional\")\n",
        "    print(\"- Menor interpretabilidade\")\n",
        "elif 'Prophet' in best_model_final:\n",
        "    print(\"- Pode ser lento com grandes volumes de dados\")\n",
        "    print(\"- Requer ajuste cuidadoso de hiperpar√¢metros\")\n",
        "elif 'ARIMA' in best_model_final or 'SARIMA' in best_model_final:\n",
        "    print(\"- Requer s√©rie estacion√°ria (ou diferencia√ß√£o)\")\n",
        "    print(\"- Pode ter dificuldade com padr√µes n√£o-lineares\")\n",
        "else:\n",
        "    print(\"- N√£o captura padr√µes complexos\")\n",
        "    print(\"- Limitado para s√©ries com m√∫ltiplas sazonalidades\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Etapa 5: Dashboard e Visualiza√ß√µes Finais\n",
        "\n",
        "## 5.1. Dashboard Interativo com Plotly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar dashboard interativo\n",
        "fig = make_subplots(\n",
        "    rows=3, cols=2,\n",
        "    subplot_titles=('S√©rie Temporal Completa', 'Temperatura M√©dia Mensal',\n",
        "                   'Compara√ß√£o de Modelos - MAE', 'Compara√ß√£o de Modelos - R¬≤',\n",
        "                   'Previs√µes vs. Real (Melhor Modelo)', 'Distribui√ß√£o de Temperatura'),\n",
        "    specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "           [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
        "           [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
        ")\n",
        "\n",
        "# 1. S√©rie temporal completa\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=df_ts.index, y=df_ts['temperature'], \n",
        "               mode='lines', name='Temperatura',\n",
        "               line=dict(color='red', width=1)),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# 2. Temperatura m√©dia mensal\n",
        "monthly_temp_plot = df_ts.groupby('month')['temperature'].mean()\n",
        "fig.add_trace(\n",
        "    go.Scatter(x=monthly_temp_plot.index, y=monthly_temp_plot.values,\n",
        "               mode='lines+markers', name='Temp. M√©dia Mensal',\n",
        "               line=dict(color='blue', width=2),\n",
        "               marker=dict(size=8)),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# 3. Compara√ß√£o MAE\n",
        "fig.add_trace(\n",
        "    go.Bar(x=valid_models['Modelo'], y=valid_models['MAE (Test)'],\n",
        "           name='MAE', marker_color='skyblue'),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# 4. Compara√ß√£o R¬≤\n",
        "fig.add_trace(\n",
        "    go.Bar(x=valid_models['Modelo'], y=valid_models['R¬≤ (Test)'],\n",
        "           name='R¬≤', marker_color='lightgreen'),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "# 5. Previs√µes vs. Real (melhor modelo)\n",
        "if best_model_final in predictions and 'test' in predictions[best_model_final]:\n",
        "    best_pred_plot = predictions[best_model_final]['test']\n",
        "    if len(best_pred_plot) > len(y_test):\n",
        "        best_pred_plot = best_pred_plot[:len(y_test)]\n",
        "    elif len(best_pred_plot) < len(y_test):\n",
        "        y_test_plot = y_test[:len(best_pred_plot)]\n",
        "    else:\n",
        "        y_test_plot = y_test\n",
        "    \n",
        "    if len(best_pred_plot) == len(y_test_plot):\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=y_test_plot.index, y=y_test_plot.values,\n",
        "                      mode='lines', name='Valores Reais',\n",
        "                      line=dict(color='blue', width=1.5)),\n",
        "            row=3, col=1\n",
        "        )\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=y_test_plot.index, y=best_pred_plot,\n",
        "                      mode='lines', name='Previs√µes',\n",
        "                      line=dict(color='red', width=1.5, dash='dash')),\n",
        "            row=3, col=1\n",
        "        )\n",
        "\n",
        "# 6. Distribui√ß√£o de temperatura\n",
        "fig.add_trace(\n",
        "    go.Histogram(x=df_ts['temperature'], nbinsx=50, name='Distribui√ß√£o',\n",
        "                marker_color='orange', opacity=0.7),\n",
        "    row=3, col=2\n",
        ")\n",
        "\n",
        "# Atualizar layout\n",
        "fig.update_layout(\n",
        "    height=1200,\n",
        "    title_text=\"Dashboard de An√°lise de S√©ries Temporais - Dados Clim√°ticos NOAA\",\n",
        "    title_x=0.5,\n",
        "    showlegend=True\n",
        ")\n",
        "\n",
        "# Atualizar eixos\n",
        "fig.update_xaxes(title_text=\"Data\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Temperatura (¬∞C)\", row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"M√™s\", row=1, col=2)\n",
        "fig.update_yaxes(title_text=\"Temperatura M√©dia (¬∞C)\", row=1, col=2)\n",
        "fig.update_xaxes(title_text=\"Modelo\", row=2, col=1)\n",
        "fig.update_yaxes(title_text=\"MAE\", row=2, col=1)\n",
        "fig.update_xaxes(title_text=\"Modelo\", row=2, col=2)\n",
        "fig.update_yaxes(title_text=\"R¬≤\", row=2, col=2)\n",
        "fig.update_xaxes(title_text=\"Data\", row=3, col=1)\n",
        "fig.update_yaxes(title_text=\"Temperatura (¬∞C)\", row=3, col=1)\n",
        "fig.update_xaxes(title_text=\"Temperatura (¬∞C)\", row=3, col=2)\n",
        "fig.update_yaxes(title_text=\"Frequ√™ncia\", row=3, col=2)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Salvar dashboard\n",
        "fig.write_html(\"dashboard_clima_noaa.html\")\n",
        "print(\"\\nDashboard salvo em 'dashboard_clima_noaa.html'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2. Previs√µes Futuras\n",
        "\n",
        "Vamos gerar previs√µes para os pr√≥ximos per√≠odos usando o melhor modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gerar previs√µes futuras (pr√≥ximos 365 dias)\n",
        "future_days = 365\n",
        "future_dates = pd.date_range(start=df_ts.index.max() + timedelta(days=1), \n",
        "                            periods=future_days, freq='D')\n",
        "\n",
        "print(f\"Gerando previs√µes para os pr√≥ximos {future_days} dias...\")\n",
        "print(f\"Per√≠odo: {future_dates.min()} a {future_dates.max()}\")\n",
        "\n",
        "# Usar o melhor modelo para previs√µes futuras\n",
        "# Exemplo com ARIMA (ajustar conforme o melhor modelo)\n",
        "if best_model_final == 'ARIMA':\n",
        "    # Re-treinar com todos os dados dispon√≠veis\n",
        "    y_all = pd.concat([y_train, y_val, y_test])\n",
        "    arima_model_future = ARIMA(y_all, order=arima_order)\n",
        "    arima_fitted_future = arima_model_future.fit()\n",
        "    future_predictions = arima_fitted_future.forecast(steps=future_days)\n",
        "    \n",
        "elif best_model_final == 'LSTM':\n",
        "    # Para LSTM, usar a √∫ltima sequ√™ncia e fazer previs√µes iterativas\n",
        "    last_sequence = train_val_combined[-seq_length:].reshape(1, seq_length, 1)\n",
        "    future_predictions_lstm = []\n",
        "    current_seq = last_sequence.copy()\n",
        "    \n",
        "    for _ in range(future_days):\n",
        "        next_pred = lstm_model.predict(current_seq, verbose=0)\n",
        "        future_predictions_lstm.append(next_pred[0, 0])\n",
        "        # Atualizar sequ√™ncia\n",
        "        current_seq = np.append(current_seq[:, 1:, :], next_pred.reshape(1, 1, 1), axis=1)\n",
        "    \n",
        "    future_predictions = scaler_lstm.inverse_transform(\n",
        "        np.array(future_predictions_lstm).reshape(-1, 1)\n",
        "    ).flatten()\n",
        "    \n",
        "elif 'Prophet' in best_model_final:\n",
        "    # Prophet\n",
        "    prophet_all = pd.DataFrame({\n",
        "        'ds': pd.concat([y_train, y_val, y_test]).index,\n",
        "        'y': pd.concat([y_train, y_val, y_test]).values\n",
        "    })\n",
        "    prophet_model_future = Prophet(\n",
        "        yearly_seasonality=True,\n",
        "        weekly_seasonality=True,\n",
        "        daily_seasonality=False\n",
        "    )\n",
        "    prophet_model_future.fit(prophet_all)\n",
        "    prophet_future_df = prophet_model_future.make_future_dataframe(periods=future_days)\n",
        "    prophet_forecast_future = prophet_model_future.predict(prophet_future_df)\n",
        "    future_predictions = prophet_forecast_future.tail(future_days)['yhat'].values\n",
        "    \n",
        "else:\n",
        "    # Para outros modelos, usar m√©dia ou √∫ltimo valor\n",
        "    y_all = pd.concat([y_train, y_val, y_test])\n",
        "    future_predictions = np.full(future_days, y_all.mean())\n",
        "\n",
        "# Visualizar previs√µes futuras\n",
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "# √öltimos 365 dias dos dados reais\n",
        "last_year = df_ts['temperature'].tail(365)\n",
        "ax.plot(last_year.index, last_year.values, label='Dados Hist√≥ricos (√∫ltimo ano)', \n",
        "        linewidth=1.5, color='blue', alpha=0.7)\n",
        "\n",
        "# Previs√µes futuras\n",
        "ax.plot(future_dates, future_predictions, label='Previs√µes Futuras', \n",
        "        linewidth=1.5, color='red', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Linha de conex√£o\n",
        "ax.plot([last_year.index.max(), future_dates.min()], \n",
        "        [last_year.values[-1], future_predictions[0]], \n",
        "        color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
        "\n",
        "ax.set_title(f'Previs√µes Futuras - {best_model_final} (Pr√≥ximos {future_days} dias)', \n",
        "            fontsize=14, fontweight='bold')\n",
        "ax.set_ylabel('Temperatura (¬∞C)')\n",
        "ax.set_xlabel('Data')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nEstat√≠sticas das previs√µes futuras:\")\n",
        "print(f\"  Temperatura m√©dia prevista: {np.mean(future_predictions):.2f} ¬∞C\")\n",
        "print(f\"  Temperatura m√≠nima prevista: {np.min(future_predictions):.2f} ¬∞C\")\n",
        "print(f\"  Temperatura m√°xima prevista: {np.max(future_predictions):.2f} ¬∞C\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclus√µes e Resumo Final\n",
        "\n",
        "## Resumo do Projeto\n",
        "\n",
        "Este projeto realizou uma an√°lise completa de s√©ries temporais de dados clim√°ticos da NOAA para S√£o Paulo, Brasil, incluindo:\n",
        "\n",
        "1. **Coleta e Explora√ß√£o**: Carregamento e processamento de dados PSV locais, an√°lise explorat√≥ria dos dados, identifica√ß√£o de padr√µes temporais, tend√™ncias e sazonalidade\n",
        "2. **Prepara√ß√£o**: Limpeza de dados, agrega√ß√£o de dados hor√°rios em di√°rios, cria√ß√£o de features temporais e divis√£o adequada dos dados\n",
        "3. **Modelagem**: Implementa√ß√£o de 8 modelos diferentes (3 baseline + 5 avan√ßados)\n",
        "4. **Avalia√ß√£o**: Compara√ß√£o sistem√°tica usando m√∫ltiplas m√©tricas (MAE, RMSE, MAPE, R¬≤)\n",
        "5. **Visualiza√ß√£o**: Dashboard interativo e visualiza√ß√µes detalhadas\n",
        "\n",
        "## Dados Utilizados\n",
        "\n",
        "- **Fonte**: NOAA GHCNh (Global Historical Climatology Network - Hourly)\n",
        "- **Localiza√ß√£o**: S√£o Paulo, Brasil\n",
        "- **Formato**: PSV (Pipe-Separated Values) - Dados hor√°rios agregados em di√°rios\n",
        "- **Processamento**: Agrega√ß√£o autom√°tica de dados hor√°rios para an√°lise di√°ria\n",
        "\n",
        "## Principais Descobertas\n",
        "\n",
        "- **Melhor Modelo**: O modelo selecionado apresentou bom desempenho nas m√©tricas de avalia√ß√£o\n",
        "- **Padr√µes Identificados**: Sazonalidade anual clara, tend√™ncia de aquecimento ao longo dos anos\n",
        "- **Desafios**: S√©rie n√£o estacion√°ria, requerendo diferencia√ß√£o ou transforma√ß√µes\n",
        "\n",
        "## Pr√≥ximos Passos Sugeridos\n",
        "\n",
        "1. Incorporar vari√°veis externas (umidade, press√£o, precipita√ß√£o, etc.) do arquivo PSV\n",
        "2. Testar ensemble de modelos para melhorar previs√µes\n",
        "3. Implementar valida√ß√£o cruzada temporal mais robusta\n",
        "4. Adicionar intervalos de confian√ßa √†s previs√µes\n",
        "5. Expandir per√≠odo de dados com mais arquivos PSV hist√≥ricos\n",
        "\n",
        "---\n",
        "\n",
        "**Projeto desenvolvido para avalia√ß√£o de compet√™ncias em An√°lise de S√©ries Temporais**\n",
        "**Dados Clim√°ticos NOAA - S√£o Paulo, Brasil**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
